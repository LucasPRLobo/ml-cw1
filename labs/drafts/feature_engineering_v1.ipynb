{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f686530",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer                                                                                                                                                                 \n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler                                                                                                                                               \n",
    "from sklearn.pipeline import Pipeline                                                                                                                                                                         \n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures, FunctionTransformer\n",
    "\n",
    "RANDOM = 123\n",
    "np.random.seed(RANDOM)\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62e0f0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../data/CW1_train.csv\")\n",
    "X = train.drop(columns=[\"outcome\"])\n",
    "y = train[\"outcome\"]\n",
    "\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=RANDOM)\n",
    "\n",
    "baseline_model = GradientBoostingRegressor(n_estimators=200, random_state=RANDOM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3420c485",
   "metadata": {},
   "source": [
    "# Test Approaches: dropping redundant features vs. creating composite features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83b2a9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline (all features): 0.4686 ± 0.0175\n"
     ]
    }
   ],
   "source": [
    "#Baseline \n",
    "                                                                                                                                                                    \n",
    "preprocessor_baseline = ColumnTransformer([                                                                                                                                                                   \n",
    "    ('cat', OneHotEncoder(drop='first', sparse_output=False), categorical_cols),                                                                                                                              \n",
    "    ('num', 'passthrough', numerical_cols)                                                                                                                                                                      \n",
    "])       \n",
    "\n",
    "pipeline_baseline = Pipeline([(\"prep\", preprocessor_baseline), (\"model\", baseline_model)])\n",
    "scores_baseline = cross_val_score(pipeline_baseline, X, y, cv=cv, scoring='r2', n_jobs=-1)\n",
    "print(f'Baseline (all features): {scores_baseline.mean():.4f} ± {scores_baseline.std():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "353e9ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced Features (dropped ['price', 'x', 'y', 'z']): 0.4700 ± 0.0175\n"
     ]
    }
   ],
   "source": [
    "# Drop Redundant Features\n",
    "drop_cols = ['price', 'x', 'y', 'z']\n",
    "numerical_cols_reduced = [col for col in numerical_cols if col not in drop_cols]\n",
    "\n",
    "preprocessor_reduced = ColumnTransformer([                                                                                                                                                                   \n",
    "    ('cat', OneHotEncoder(drop='first', sparse_output=False), categorical_cols),                                                                                                                              \n",
    "    ('num', 'passthrough', numerical_cols_reduced)                                                                                                                                                                      \n",
    "])\n",
    "pipeline_reduced = Pipeline([(\"prep\", preprocessor_reduced), (\"model\", baseline_model)])\n",
    "scores_reduced = cross_val_score(pipeline_reduced, X, y, cv=cv, scoring='r2', n_jobs=-1)\n",
    "print(f'Reduced Features (dropped {drop_cols}): {scores_reduced.mean():.4f} ± {scores_reduced.std():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "078af7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Composite Features (added volume & price_per_carat, dropped ['price', 'x', 'y', 'z']): 0.4688 ± 0.0165\n"
     ]
    }
   ],
   "source": [
    "# Composite Features\n",
    "X_composite = X.copy()  \n",
    "X_composite['volume'] = X['x'] * X['y'] * X['z']\n",
    "X_composite[\"price_per_carat\"] = X[\"price\"] / (X[\"carat\"] + 1e-6)\n",
    "\n",
    "drop_cols_composite = ['price', 'x', 'y', 'z']\n",
    "numeric_composite = [col for col in X_composite.columns                                                                                                                                                       \n",
    "                       if col not in categorical_cols and col not in drop_cols_composite]    \n",
    "\n",
    "preprocessor_composite = ColumnTransformer([                                                                                                                                                                   \n",
    "    ('cat', OneHotEncoder(drop='first', sparse_output=False), categorical_cols),                                                                                                                              \n",
    "    ('num', 'passthrough', numeric_composite)                                                                                                                                                                      \n",
    "])\n",
    "\n",
    "pipeline_composite = Pipeline([(\"prep\", preprocessor_composite), (\"model\", baseline_model)])\n",
    "scores_composite = cross_val_score(pipeline_composite, X_composite, y, cv=cv, scoring='r2', n_jobs=-1)\n",
    "print(f'Composite Features (added volume & price_per_carat, dropped {drop_cols_composite}): {scores_composite.mean():.4f} ± {scores_composite.std():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8106f190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MULTICOLLINEARITY HANDLING STRATEGY COMPARISON:\n",
      "Baseline (all features):      0.4686 ± 0.0175\n",
      "Reduced (drop redundant):     0.4700 ± 0.0175\n",
      "Composite (volume, ppc):      0.4688 ± 0.0165\n"
     ]
    }
   ],
   "source": [
    "# Summary\n",
    "print(\"\\nMULTICOLLINEARITY HANDLING STRATEGY COMPARISON:\")\n",
    "print(f\"Baseline (all features):      {scores_baseline.mean():.4f} ± {scores_baseline.std():.4f}\")                                                                                                            \n",
    "print(f\"Reduced (drop redundant):     {scores_reduced.mean():.4f} ± {scores_reduced.std():.4f}\")                                                                                                              \n",
    "print(f\"Composite (volume, ppc):      {scores_composite.mean():.4f} ± {scores_composite.std():.4f}\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f8a747",
   "metadata": {},
   "source": [
    "Takeaways:                                                                                                                                                                                                    \n",
    "  - Dropping redundant features (price, x, y, z) gives a small but consistent improvement                                                                                                                       \n",
    "  - Composite features didn't add value - the model already captures the relationships                                                                                                                          \n",
    "  - Tree-based models handle multicollinearity well, but less noise still helps                                                                                                                                 \n",
    "                                                                                                                                                                                                                \n",
    "  Decision: Use the reduced feature set going forward.                     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e03cd99",
   "metadata": {},
   "source": [
    "# Latent Feature Interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1ee78c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent Feature Interactions: 0.4703 ± 0.0185\n",
      "vs Reduced baseline: 0.4700 ± 0.0175\n"
     ]
    }
   ],
   "source": [
    "X_reduced = X.drop(columns=['price', 'x', 'y', 'z'])\n",
    "\n",
    "# Latent Feature Groups\n",
    "latent_uniform = ['a1', 'a2', 'a3', 'a4', 'a5', 'b1', 'b2', 'b3', 'b4', 'b5']                                                                                                                                 \n",
    "latent_gaussian = ['a6', 'a7', 'a8', 'a9', 'a10', 'b6', 'b7', 'b8', 'b9', 'b10']  \n",
    "\n",
    "X_interactions = X_reduced.copy()\n",
    "\n",
    "# Cross group interactions\n",
    "for i in range(6, 11):\n",
    "    X_interactions[f'a{i}_x_b{i}'] = X_reduced[f'a{i}'] * X_reduced[f'b{i}']     \n",
    "\n",
    "# Within group interactions\n",
    "X_interactions['a6_x_a7'] = X_reduced['a6'] * X_reduced['a7']                                                                                                                                                 \n",
    "X_interactions['b6_x_b7'] = X_reduced['b6'] * X_reduced['b7'] \n",
    "\n",
    "numeric_interactions = [col for col in X_interactions.columns if col not in categorical_cols]\n",
    "\n",
    "preprocessor_interactions = ColumnTransformer([                                                                                                                                                                   \n",
    "    ('cat', OneHotEncoder(drop='first', sparse_output=False), categorical_cols),                                                                                                                              \n",
    "    ('num', 'passthrough', numeric_interactions)                                                                                                                                                                      \n",
    "])\n",
    "\n",
    "pipeline_interactions = Pipeline([(\"prep\", preprocessor_interactions), (\"model\", baseline_model)])\n",
    "scores_interactions = cross_val_score(pipeline_interactions, X_interactions, y, cv=cv, scoring='r2', n_jobs=-1)\n",
    "print(f'Latent Feature Interactions: {scores_interactions.mean():.4f} ± {scores_interactions.std():.4f}')\n",
    "print(f\"vs Reduced baseline: {scores_reduced.mean():.4f} ± {scores_reduced.std():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd99b932",
   "metadata": {},
   "source": [
    "  Takeaway: \n",
    "  Essentially no improvement. The increase in std (0.0185 vs 0.0175) suggests added noise rather than signal. GradientBoosting already captures these interactions internally through its tree splits.\n",
    "                                                                                                                                                                                                                \n",
    "  Decision: Skip explicit interactions - not worth the added complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76cefa6",
   "metadata": {},
   "source": [
    "# Polynomial Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eca6d8db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polynomial (degree=2) on Gaussian latents: 0.4645 ± 0.0179\n",
      "vs Reduced baseline: 0.4700 ± 0.0175\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures                                                                                                                                                          \n",
    "                                                                                                                                                                                                            \n",
    "                                                                                                                                                                              \n",
    "X_reduced = X.drop(columns=['price', 'x', 'y', 'z'])                                                                                                                                                          \n",
    "                                                                                                                                                                                                            \n",
    "# Separate latent Gaussian for polynomial expansion                                                                                                                                                           \n",
    "latent_gaussian = ['a6', 'a7', 'a8', 'a9', 'a10', 'b6', 'b7', 'b8', 'b9', 'b10']                                                                                                                              \n",
    "other_numeric = [col for col in X_reduced.columns                                                                                                                                                             \n",
    "                if col not in categorical_cols and col not in latent_gaussian]                                                                                                                               \n",
    "                                                                                                                                                                                                            \n",
    "# Preprocessor with polynomial features on Gaussian latents only                                                                                                                                              \n",
    "preprocessor_poly = ColumnTransformer([                                                                                                                                                                       \n",
    "    ('cat', OneHotEncoder(drop='first', sparse_output=False), categorical_cols),                                                                                                                              \n",
    "    ('poly', PolynomialFeatures(degree=2, include_bias=False), latent_gaussian),                                                                                                                              \n",
    "    ('num', 'passthrough', other_numeric)                                                                                                                                                                     \n",
    "])                                                                                                                                                                                                            \n",
    "                                                                                                                                                                                                            \n",
    "pipe_poly = Pipeline([('prep', preprocessor_poly), ('model', baseline_model)])                                                                                                                                \n",
    "scores_poly = cross_val_score(pipe_poly, X_reduced, y, cv=cv, scoring='r2', n_jobs=-1)                                                                                                                        \n",
    "print(f\"Polynomial (degree=2) on Gaussian latents: {scores_poly.mean():.4f} ± {scores_poly.std():.4f}\")                                                                                                       \n",
    "print(f\"vs Reduced baseline: {scores_reduced.mean():.4f} ± {scores_reduced.std():.4f}\")         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769efaff",
   "metadata": {},
   "source": [
    "  Takeaway: Polynomial features hurt performance. The expansion (10 features → 65 features with degree 2) likely added noise and caused the model to overfit on spurious patterns. Tree-based models already    \n",
    "  capture nonlinearities through splits.                                                                                                                                                                        \n",
    "                                                                                                                                                                                                                \n",
    "  Decision: Skip polynomial features.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da213701",
   "metadata": {},
   "source": [
    "# Target Transformation\n",
    "EDA showed heteroskedasticity (errors increase for extreme values). A target transformation might stabilize variance and improve predictions.      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcdabf7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TARGET TRANSFORMATION RESULTS:\n",
      "No transformation:  0.4700 ± 0.0175\n",
      "Shifted log:        0.4524 ± 0.0173\n",
      "Signed sqrt:        0.4145 ± 0.0112\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import TransformedTargetRegressor\n",
    "\n",
    "X_reduced = X.drop(columns=['price', 'x', 'y', 'z'])\n",
    "numeric_reduced = [col for col in X_reduced.columns if col not in categorical_cols]\n",
    "\n",
    "preprocessor_reduced = ColumnTransformer([                                                                                                                                                                   \n",
    "    ('cat', OneHotEncoder(drop='first', sparse_output=False), categorical_cols),                                                                                                                              \n",
    "    ('num', 'passthrough', numeric_reduced)                                                                                                                                                                      \n",
    "])\n",
    "\n",
    "# Since target has negative values, we can't use log directly                                                                                                                                                 \n",
    "# Option 1: Shift + log                                                                                                                                                                                       \n",
    "y_min = y.min()                                                                                                                                                                                               \n",
    "shift = abs(y_min) + 1  # Ensure all values positive       \n",
    "\n",
    "def shift_log(y):                                                                                                                                                                                             \n",
    "      return np.log(y + shift)                                                                                                                                                                                  \n",
    "                                                                                                                                                                                                            \n",
    "def shift_exp(y):                                                                                                                                                                                             \n",
    "    return np.exp(y) - shift                                                                                                                                                                                  \n",
    "                                                                                                                                                                                                            \n",
    "# Option 2: sqrt with sign preservation                                                                                                                                                                       \n",
    "def signed_sqrt(y):                                                                                                                                                                                           \n",
    "    return np.sign(y) * np.sqrt(np.abs(y))                                                                                                                                                                    \n",
    "                                                                                                                                                                                                            \n",
    "def signed_square(y):                                                                                                                                                                                         \n",
    "    return np.sign(y) * (y ** 2)\n",
    "\n",
    "# Choose transformation\n",
    "# Test 1: No transformation (baseline)                                                                                                                                                                        \n",
    "pipe_no_transform = Pipeline([('prep', preprocessor_reduced), ('model', baseline_model)])                                                                                                                     \n",
    "scores_no_transform = cross_val_score(pipe_no_transform, X_reduced, y, cv=cv, scoring='r2', n_jobs=-1)                                                                                                        \n",
    "                                                                                                                                                                                                            \n",
    "# Test 2: Shifted log transformation                                                                                                                                                                          \n",
    "model_log = TransformedTargetRegressor(                                                                                                                                                                       \n",
    "    regressor=Pipeline([('prep', preprocessor_reduced), ('model', baseline_model)]),                                                                                                                          \n",
    "    func=shift_log,                                                                                                                                                                                           \n",
    "    inverse_func=shift_exp                                                                                                                                                                                    \n",
    ")                                                                                                                                                                                                             \n",
    "scores_log = cross_val_score(model_log, X_reduced, y, cv=cv, scoring='r2', n_jobs=-1)                                                                                                                         \n",
    "                                                                                                                                                                                                            \n",
    "# Test 3: Signed sqrt transformation                                                                                                                                                                          \n",
    "model_sqrt = TransformedTargetRegressor(                                                                                                                                                                      \n",
    "    regressor=Pipeline([('prep', preprocessor_reduced), ('model', baseline_model)]),                                                                                                                          \n",
    "    func=signed_sqrt,                                                                                                                                                                                         \n",
    "    inverse_func=signed_square                                                                                                                                                                                \n",
    ")                                                                                                                                                                                                             \n",
    "scores_sqrt = cross_val_score(model_sqrt, X_reduced, y, cv=cv, scoring='r2', n_jobs=-1)                                                                                                                       \n",
    "                                                                                                                                                                                                            \n",
    "print(\"TARGET TRANSFORMATION RESULTS:\")                                                                                                                                                                       \n",
    "print(f\"No transformation:  {scores_no_transform.mean():.4f} ± {scores_no_transform.std():.4f}\")                                                                                                              \n",
    "print(f\"Shifted log:        {scores_log.mean():.4f} ± {scores_log.std():.4f}\")                                                                                                                                \n",
    "print(f\"Signed sqrt:        {scores_sqrt.mean():.4f} ± {scores_sqrt.std():.4f}\")             \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b63216a",
   "metadata": {},
   "source": [
    "  Takeaway: Target transformations significantly hurt performance. The model handles the original target distribution well. Transformations distort the relationships GradientBoosting was learning effectively.\n",
    "                                                                                                                                                                                                                \n",
    "  Decision: Keep target untransformed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832f5df9",
   "metadata": {},
   "source": [
    "  Key insight: Feature engineering provided minimal gains. GradientBoosting already handles multicollinearity, interactions, and nonlinearity through its tree structure. Only removing redundant features      \n",
    "  helped slightly.                                                                                                                                                                                              \n",
    "                                                                                                                                                                                                                \n",
    "  Best config entering Phase 5:                                                                                                                                                                                 \n",
    "  - Model: GradientBoostingRegressor                                                                                                                                                                            \n",
    "  - Features: Reduced set (drop price, x, y, z)                                                                                                                                                                 \n",
    "  - CV R²: 0.4700 ± 0.0175    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c82e1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures                                                                                                         \n",
    "import numpy as np            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfe81f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4bb520",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_uniform = ['a1', 'a2', 'a3', 'a4', 'a5', 'b1', 'b2', 'b3', 'b4', 'b5']                                                                                \n",
    "latent_gaussian = ['a6', 'a7', 'a8', 'a9', 'a10', 'b6', 'b7', 'b8', 'b9', 'b10']                                                                             \n",
    "                                                                                                                                                            \n",
    "# Create engineered features                                                                                                                                 \n",
    "X_eng = X.copy()                                                                                                                                             \n",
    "                                                                                                                                                            \n",
    "# 1. Interactions within latent groups (a*b pairs)                                                                                                           \n",
    "for i in range(1, 11):                                                                                                                                       \n",
    "    X_eng[f'ab_{i}'] = X[f'a{i}'] * X[f'b{i}']                                                                                                               \n",
    "                                                                                                                                                            \n",
    "# 2. Sum/mean aggregations                                                                                                                                   \n",
    "X_eng['a_sum'] = X[[f'a{i}' for i in range(1,11)]].sum(axis=1)                                                                                               \n",
    "X_eng['b_sum'] = X[[f'b{i}' for i in range(1,11)]].sum(axis=1)                                                                                               \n",
    "X_eng['ab_diff'] = X_eng['a_sum'] - X_eng['b_sum']                                                                                                           \n",
    "                                                                                                                                                            \n",
    "# 3. Gaussian latent squared terms (capture non-linearity)                                                                                                   \n",
    "for col in latent_gaussian:                                                                                                                                  \n",
    "    X_eng[f'{col}_sq'] = X[col] ** 2                                                                                                                         \n",
    "                                                                                                                                                            \n",
    "print(f\"Original features: {X.shape[1]}\")                                                                                                                    \n",
    "print(f\"Engineered features: {X_eng.shape[1]}\")                                                                                                              \n",
    "                                                                                                                                                            \n",
    "# Update column lists                                                                                                                                        \n",
    "cat_cols_eng = cat_cols                                                                                                                                      \n",
    "num_cols_eng = [c for c in X_eng.columns if c not in cat_cols]                                                                                               \n",
    "                                                                                                                                                            \n",
    "# New preprocessor                                                                                                                                           \n",
    "preprocessor_eng = ColumnTransformer([                                                                                                                       \n",
    "    ('cat', OneHotEncoder(drop='first', sparse_output=False), cat_cols_eng),                                                                                 \n",
    "    ('num', 'passthrough', num_cols_eng)                                                                                                                     \n",
    "])                                                                                                                                                           \n",
    "                                                                                                                                                            \n",
    "# Test with LightGBM (fastest)                                                                                                                               \n",
    "pipe_lgb_eng = Pipeline([                                                                                                                                    \n",
    "    ('prep', preprocessor_eng),                                                                                                                              \n",
    "    ('model', LGBMRegressor(**search_lgb.best_params_, random_state=RANDOM, n_jobs=-1, verbose=-1))                                                          \n",
    "])                                                                                                                                                           \n",
    "                                                                                                                                                            \n",
    "# Remove 'model__' prefix from params                                                                                                                        \n",
    "lgb_params = {k.replace('model__', ''): v for k, v in search_lgb.best_params_.items()}                                                                       \n",
    "pipe_lgb_eng = Pipeline([                                                                                                                                    \n",
    "    ('prep', preprocessor_eng),                                                                                                                              \n",
    "    ('model', LGBMRegressor(**lgb_params, random_state=RANDOM, n_jobs=-1, verbose=-1))                                                                       \n",
    "])                                                                                                                                                           \n",
    "                                                                                                                                                            \n",
    "print(\"\\nEvaluating LightGBM with engineered features...\")                                                                                                   \n",
    "start = time.time()                                                                                                                                          \n",
    "scores = cross_val_score(pipe_lgb_eng, X_eng, y, cv=cv, scoring='r2', n_jobs=-1)                                                                             \n",
    "elapsed = time.time() - start                                                                                                                                \n",
    "                                                                                                                                                            \n",
    "print(f\"Completed in {elapsed:.1f}s\")                                                                                                                        \n",
    "print(f\"LightGBM + Engineered CV R²: {scores.mean():.4f} ± {scores.std():.4f}\")                                                                              \n",
    "print(f\"vs baseline LightGBM: 0.4753\")           "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
